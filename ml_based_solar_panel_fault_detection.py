# -*- coding: utf-8 -*-
"""ML Based Solar Panel Fault Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ugcJFxjy9LqOdscZ3qw2It0Irhk8XxN5
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q scikit-learn pandas matplotlib joblib fastapi uvicorn pyngrok nest_asyncio pyfcm prophet streamlit tensorflow==2.17.1
import os
os.makedirs('/content/models', exist_ok=True)
print("Setup complete")

import pandas as pd, random
from datetime import datetime
def generate_stream_rows(n=600):
    rows=[]
    for i in range(n):
        ts=(datetime.utcnow()).isoformat()
        irr = max(50, 800 + random.randint(-50,50))
        amb = 25 + random.random()*10
        panel_temp = amb + random.random()*10
        volt = 18.5 + random.uniform(-1,1)
        curr = 0.9 + random.uniform(-0.4,0.1)
        power = round(volt*curr,3)
        label='Normal'
        if i%150==100:
            curr*=0.5; power=round(volt*curr,3); label='Shading'
        if i%220==200:
            curr*=0.6; power=round(volt*curr,3); label='Soiling'
        rows.append([ts,irr,amb,panel_temp,volt,curr,power,label])
    df=pd.DataFrame(rows, columns=['timestamp','irradiance_wpm2','ambient_temp_c','panel_temp_c','voltage_v','current_a','power_w','label'])
    df.to_csv('/content/sim_stream.csv', index=False)
    return '/content/sim_stream.csv'

csv_path = generate_stream_rows()
pd.read_csv(csv_path).head()

import pandas as pd, numpy as np
def add_features(df):
    df = df.copy()
    df['power_w'] = df['voltage_v'] * df['current_a']
    df['efficiency'] = df['power_w'] / (df['irradiance_wpm2'] + 1e-6)
    df['temp_delta'] = df['panel_temp_c'] - df['ambient_temp_c']
    df['rolling_power_30s'] = df['power_w'].rolling(window=30, min_periods=1).mean()
    df['residual_power'] = df['power_w'] - df['rolling_power_30s']
    df.fillna(method='ffill', inplace=True); df.fillna(0, inplace=True)
    return df

df = pd.read_csv('/content/sim_stream.csv', parse_dates=['timestamp'])
df = add_features(df)
df.tail()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import joblib, matplotlib.pyplot as plt

features = ['irradiance_wpm2','panel_temp_c','voltage_v','current_a','power_w','efficiency','temp_delta','residual_power']
X = df[features]; y = df['label']
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, stratify=y, random_state=42)

clf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)
clf.fit(X_train, y_train)
preds = clf.predict(X_test)
print(classification_report(y_test, preds))
joblib.dump(clf, '/content/models/tabular_rf.pkl')

cm = confusion_matrix(y_test, preds, labels=clf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax)
plt.title('Confusion Matrix — Tabular Model'); plt.show()

from prophet import Prophet
df_pm = df[['timestamp','efficiency']].rename(columns={'timestamp':'ds','efficiency':'y'})
m = Prophet()
m.fit(df_pm)
future = m.make_future_dataframe(periods=30, freq='T')
forecast = m.predict(future)
m.plot(forecast);

current_eff = df_pm['y'].tail(60).mean()
pred_eff = forecast['yhat'].iloc[-1]
print("Current eff:", current_eff, "Pred eff:", pred_eff)
if pred_eff < current_eff * 0.9:
    print("⚠️ Predictive maintenance alert: predicted efficiency drop detected")
else:
    print("No immediate maintenance predicted")

import numpy as np
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models

def generate_image(label, size=64, rng=None):
    if rng is None: rng=np.random.RandomState()
    base = rng.normal(loc=0.5, scale=0.05, size=(size,size))
    if label=='Soiling':
        for _ in range(3):
            x=rng.randint(0,size-10); y=rng.randint(0,size-10)
            w=rng.randint(8,18); h=rng.randint(8,18)
            base[y:y+h,x:x+w]-=rng.uniform(0.15,0.3)
    elif label=='Shading':
        band=rng.randint(size//4,size//2)
        if rng.rand()>0.5: base[:band,:]-=rng.uniform(0.2,0.35)
        else: base[-band:,:]-=rng.uniform(0.2,0.35)
    elif label=='Hotspot':
        x=rng.randint(size//4,3*size//4); y=rng.randint(size//4,3*size//4)
        for i in range(-4,5):
            for j in range(-4,5):
                if 0<=y+i<size and 0<=x+j<size:
                    base[y+i,x+j]+=np.exp(-(i*i+j*j)/8.0)*rng.uniform(0.4,0.8)
    base=np.clip(base,0.0,1.0)
    return (base*255).astype('uint8')

classes = ['Normal','Soiling','Shading','Hotspot']
n_per_class = 300
imgs=[]; lbls=[]
rng=np.random.RandomState(0)
for c in classes:
    for i in range(n_per_class):
        imgs.append(generate_image(c, size=64, rng=rng))
        lbls.append(c)
imgs = np.array(imgs)[...,np.newaxis]/255.0
le = LabelEncoder(); y_img = le.fit_transform(lbls)
from sklearn.model_selection import train_test_split
Xtr,Xte,ytr,yte = train_test_split(imgs, y_img, test_size=0.2, stratify=y_img, random_state=42)

def create_cnn(input_shape, n_classes):
    m = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Conv2D(16,3,activation='relu'), layers.MaxPool2D(2),
        layers.Conv2D(32,3,activation='relu'), layers.MaxPool2D(2),
        layers.Flatten(), layers.Dense(64,activation='relu'), layers.Dropout(0.3),
        layers.Dense(n_classes,activation='softmax')
    ])
    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return m

cnn = create_cnn(Xtr.shape[1:], len(classes))
hist = cnn.fit(Xtr, ytr, validation_split=0.1, epochs=10, batch_size=64)
cnn.save('/content/models/cnn_thermal.h5')

import joblib, numpy as np
clf = joblib.load('/content/models/tabular_rf.pkl')
from tensorflow.keras.models import load_model
cnn = load_model('/content/models/cnn_thermal.h5')

def hybrid_infer(row_dict, image=None, confidence_threshold=0.7):
    feat = ['irradiance_wpm2','panel_temp_c','voltage_v','current_a','power_w','efficiency','temp_delta','residual_power']
    x = np.array([[row_dict.get(f,0) for f in feat]])
    probs = clf.predict_proba(x)[0]
    idx = probs.argmax(); conf = probs.max()
    pred = clf.classes_[idx]
    if conf < confidence_threshold and image is not None:
        img = image.astype('float32')/255.0
        img = np.expand_dims(img, axis=0)
        ip = cnn.predict(img).argmax(axis=1)[0]
        return {'source':'cnn','pred':classes[ip]}
    return {'source':'tabular','pred':pred,'conf':float(conf)}

# test example
last = df.iloc[-1]
sample = {f: float(last[f]) for f in features}
print(hybrid_infer(sample))

from pyngrok import ngrok

# replace with your token
NGROK_AUTH_TOKEN = "32uumOxLwrY6rWR4dDkOQB401qa_6g1TtdcC3A3Bhc4g2JLjz"

ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Run only once. This starts a local FastAPI server and exposes it with ngrok.
from fastapi import FastAPI
from pydantic import BaseModel
import nest_asyncio, uvicorn, threading
from pyngrok import ngrok

app = FastAPI()
class Payload(BaseModel):
    timestamp: str
    irradiance_wpm2: float
    ambient_temp_c: float
    panel_temp_c: float
    voltage_v: float
    current_a: float

@app.post('/infer')
async def infer(payload: Payload):
    row = payload.dict()
    return hybrid_infer(row)  # uses function from previous cell

def run_server():
    nest_asyncio.apply()
    uvicorn.run(app, host='0.0.0.0', port=8000)

t = threading.Thread(target=run_server, daemon=True); t.start()
public_url = ngrok.connect(8000).public_url
print("Public FastAPI URL:", public_url, " — infer endpoint:", public_url + "/infer")

import requests

url = "https://43bfe8041414.ngrok-free.app/infer"

# Example simulated data (replace with actual panel sensor values later)
data = {
    "voltage": 32.5,
    "current": 5.2,
    "temperature": 45,
    "irradiance": 850
}

response = requests.post(url, json=data)
print(response.json())

public_url = ngrok.connect(8001).public_url

from fastapi import FastAPI
from pydantic import BaseModel
import random
import datetime

app = FastAPI(title="Solar Fault Detection API")

class SensorData(BaseModel):
    voltage: float
    current: float
    temperature: float
    irradiance: float

FAULT_CLASSES = [
    "Normal Operation",
    "Short Circuit",
    "Open Circuit",
    "Shading Detected",
    "Soiling (Dust Accumulation)",
    "Degradation (Efficiency Loss)",
    "Bypass Diode Failure"
]

@app.post("/infer")
def infer(data: SensorData):
    # TODO: Replace with ML inference from trained models
    fault_class = random.choice(FAULT_CLASSES)
    confidence = round(random.uniform(0.75, 0.99), 2)

    recommendations = {
        "Normal Operation": "System is operating normally.",
        "Short Circuit": "Inspect wiring and isolate the faulty string.",
        "Open Circuit": "Check panel connections and bypass diodes.",
        "Shading Detected": "Remove shading obstacles or reconfigure panels.",
        "Soiling (Dust Accumulation)": "Clean the solar panel surface.",
        "Degradation (Efficiency Loss)": "Schedule replacement/inspection.",
        "Bypass Diode Failure": "Replace faulty bypass diode."
    }

    # Predictive maintenance (stub: next 7 days risk %)
    pred_failure_risk = round(random.uniform(0.05, 0.35), 2)

    return {
        "timestamp": datetime.datetime.utcnow(),
        "fault_class": fault_class,
        "confidence": confidence,
        "recommendation": recommendations[fault_class],
        "predictive_risk_7d": pred_failure_risk
    }

# streamlit_app.py
import streamlit as st
import joblib
import os
import numpy as np
import pandas as pd
import requests
from PIL import Image
import io
import time

st.set_page_config(page_title="Solar Fault Monitor", layout="wide")

MODEL_DIR = "models"
TABULAR_MODEL_PATH = os.path.join(MODEL_DIR, "tabular_rf.pkl")
CNN_MODEL_PATH     = os.path.join(MODEL_DIR, "cnn_thermal.h5")

DEFAULT_RAW_BASE = ""  # if you want to host models as raw files e.g. https://raw.githubusercontent.com/<user>/<repo>/main/models/

# --- Utility: download model from raw GitHub URL if not present
def download_if_missing(filename, raw_base):
    os.makedirs(MODEL_DIR, exist_ok=True)
    local = os.path.join(MODEL_DIR, filename)
    if os.path.exists(local):
        return local
    if not raw_base:
        return None
    url = raw_base.rstrip("/") + "/models/" + filename
    try:
        r = requests.get(url, stream=True, timeout=30)
        r.raise_for_status()
        with open(local, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        return local
    except Exception as e:
        st.warning(f"Could not download {filename} from {url}: {e}")
        return None

# --- Load models (tabular and CNN)
@st.cache_resource
def load_tabular_model(path):
    try:
        return joblib.load(path)
    except Exception as e:
        st.error(f"Failed to load tabular model: {e}")
        return None

@st.cache_resource
def load_cnn_model(path):
    try:
        from tensorflow.keras.models import load_model
        return load_model(path)
    except Exception as e:
        st.info("CNN model not loaded (not needed for tabular only).")
        return None

# In sidebar, let user provide raw github base URL or backend url
st.sidebar.header("Configuration")
raw_base = st.sidebar.text_input("Raw GitHub base URL (optional)", DEFAULT_RAW_BASE, help="If your models are in GitHub, put e.g. https://raw.githubusercontent.com/username/repo/main")
use_backend = st.sidebar.checkbox("Use remote inference backend (FastAPI)", value=False)
backend_url = ""
if use_backend:
    backend_url = st.sidebar.text_input("Backend /infer endpoint (include /infer)", value="", help="If you host FastAPI, enter e.g. https://your-backend.com/infer")

download_models = st.sidebar.button("Ensure models present (download if missing)")

if download_models:
    download_if_missing("tabular_rf.pkl", raw_base)
    download_if_missing("cnn_thermal.h5", raw_base)
    st.experimental_rerun()

# Try to ensure models exist (download if raw_base provided)
if not os.path.exists(TABULAR_MODEL_PATH):
    download_if_missing("tabular_rf.pkl", raw_base)
if not os.path.exists(CNN_MODEL_PATH):
    download_if_missing("cnn_thermal.h5", raw_base)

tabular_model = load_tabular_model(TABULAR_MODEL_PATH) if os.path.exists(TABULAR_MODEL_PATH) else None
cnn_model = load_cnn_model(CNN_MODEL_PATH) if os.path.exists(CNN_MODEL_PATH) else None

st.title("☀️ Solar Panel Fault Detection — Streamlit")

# UI layout: inputs left, results right
left, right = st.columns([1,1])

with left:
    st.subheader("Sensor Input (manual or CSV upload)")
    mode = st.radio("Input mode:", ["Manual single reading", "Upload CSV (batch)"], index=0)
    if mode == "Manual single reading":
        irradiance = st.number_input("Irradiance (W/m²)", value=800.0)
        ambient_temp = st.number_input("Ambient temperature (°C)", value=30.0)
        panel_temp = st.number_input("Panel temperature (°C)", value=35.0)
        voltage = st.number_input("Voltage (V)", value=18.5)
        current = st.number_input("Current (A)", value=0.85)
        if st.button("Predict (local)"):
            row = {
                "irradiance_wpm2": irradiance,
                "ambient_temp_c": ambient_temp,
                "panel_temp_c": panel_temp,
                "voltage_v": voltage,
                "current_a": current
            }
            # compute derived features
            row["power_w"] = row["voltage_v"] * row["current_a"]
            row["efficiency"] = row["power_w"]/(row["irradiance_wpm2"]+1e-6)
            row["temp_delta"] = row["panel_temp_c"] - row["ambient_temp_c"]
            # rolling/residual not available for single reading -> set 0
            row["residual_power"] = 0.0

            if use_backend and backend_url:
                # call remote inference
                try:
                    resp = requests.post(backend_url, json=row, timeout=10, verify=False)
                    st.json(resp.json())
                except Exception as e:
                    st.error(f"Remote request failed: {e}")
            else:
                if tabular_model:
                    feat_list = ['irradiance_wpm2','panel_temp_c','voltage_v','current_a','power_w','efficiency','temp_delta','residual_power']
                    x = np.array([[row[f] for f in feat_list]])
                    pred = tabular_model.predict(x)[0]
                    prob = None
                    if hasattr(tabular_model, "predict_proba"):
                        prob = tabular_model.predict_proba(x).max()
                    st.success(f"Detected: {pred} ({prob:.2f} conf)" if prob else f"Detected: {pred}")
                else:
                    st.error("Tabular model not available in models/ — upload or provide raw GitHub URL.")

    else:
        up = st.file_uploader("Upload CSV with columns (timestamp,irradiance_wpm2,ambient_temp_c,panel_temp_c,voltage_v,current_a,label(optional))", type=["csv"])
        if up is not None:
            df = pd.read_csv(up, parse_dates=["timestamp"] if "timestamp" in up.name else None)
            st.dataframe(df.head())
            if st.button("Batch predict (local)"):
                # add features like in Colab
                df['power_w'] = df['voltage_v'] * df['current_a']
                df['efficiency'] = df['power_w']/(df['irradiance_wpm2']+1e-6)
                df['temp_delta'] = df['panel_temp_c'] - df['ambient_temp_c']
                df['rolling_power_30s'] = df['power_w'].rolling(window=30, min_periods=1).mean()
                df['residual_power'] = df['power_w'] - df['rolling_power_30s']
                feat_list = ['irradiance_wpm2','panel_temp_c','voltage_v','current_a','power_w','efficiency','temp_delta','residual_power']
                if tabular_model:
                    preds = tabular_model.predict(df[feat_list])
                    df['pred'] = preds
                    st.dataframe(df.head(50))
                else:
                    st.error("Tabular model not found.")

    st.markdown("---")
    st.subheader("Optional: Thermal Image (upload)")
    img_up = st.file_uploader("Upload thermal/IR image (PNG/JPG)", type=["png","jpg","jpeg"])
    if img_up is not None:
        image = Image.open(img_up).convert("L").resize((64,64))
        st.image(image, caption="Uploaded image (grayscale)")
        arr = np.array(image)[...,np.newaxis]/255.0
        if cnn_model is not None and st.button("Predict image (CNN)"):
            p = cnn_model.predict(np.expand_dims(arr, 0)).argmax(axis=1)[0]
            st.success(f"Image class: {p} (index). Map index to labels used during training")

with right:
    st.subheader("Inference log / Output")
    if "history" not in st.session_state:
        st.session_state.history = []
    if st.button("Add test record (auto sample)"):
        sample = {
            "irradiance_wpm2": 800,
            "panel_temp_c": 35,
            "voltage_v": 18.5,
            "current_a": 0.85,
            "power_w": 18.5*0.85
        }
        sample["efficiency"] = sample["power_w"]/(sample["irradiance_wpm2"]+1e-6)
        sample["temp_delta"] = sample["panel_temp_c"] - 30
        sample["residual_power"] = 0.0
        if tabular_model:
            feat_list = ['irradiance_wpm2','panel_temp_c','voltage_v','current_a','power_w','efficiency','temp_delta','residual_power']
            x = np.array([[sample[f] for f in feat_list]])
            pred = tabular_model.predict(x)[0]
            prob = tabular_model.predict_proba(x).max() if hasattr(tabular_model, "predict_proba") else None
            entry = {"ts": time.time(), "pred": pred, "conf": float(prob) if prob is not None else None}
            st.session_state.history.append(entry)
            st.success(f"{pred}  (conf={prob:.2f})" if prob else f"{pred}")
        else:
            st.info("No tabular model loaded.")

    if st.session_state.history:
        st.write(pd.DataFrame(st.session_state.history).tail(20))
    else:
        st.info("No inferences yet. Use inputs on left and click Predict or Add test record.")

st.markdown("---")
st.write("Tip: For device integration you usually deploy a FastAPI backend to accept device HTTP/MQTT and send the data to the models. Streamlit is best for UI/monitoring (not as an API ingestion endpoint).")











































